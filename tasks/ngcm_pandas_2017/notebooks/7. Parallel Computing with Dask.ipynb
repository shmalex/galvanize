{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create several gzipped files\n",
    "* Each line in each file is a JSON encoded dictionary with the following keys\n",
    "\n",
    "```\n",
    "id: Unique identifier of the customer\n",
    "name: Name of the customer\n",
    "transactions: List of transaction-id, amount pairs, one for each transaction for the customer in that file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accounts import create_accounts_json\n",
    "\n",
    "num_files = 25\n",
    "n = 100000  # number of accounts per file\n",
    "k = 500  # number of transactions\n",
    "\n",
    "create_accounts_json(num_files, n, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denormalize NFS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The NFS data is *normalized* to eliminate redundancy\n",
    "* If you haven't, denormalize this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a billion number array of 32-bit floats on disk using HDF5\n",
    "* HDF5 is an implementation of the Hierarchical Data Format common in scientific applications\n",
    "  * Multiple data formats (tables, nd-arrays, raster images)\n",
    "  * Fast lookups via B-tree indices (like SQL)\n",
    "  * Filesystem-like data format\n",
    "  * Support for meta-information\n",
    "* The result of this operation is 4 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random_array import random_array\n",
    "random_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dask is a flexible parallel computing library for analytics. Dask emphasizes the following virtues:\n",
    "\n",
    "* **Familiar**: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "* **Native**: Enables distributed computing in Pure Python with access to the PyData stack.\n",
    "* **Fast**: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "* **Flexible**: Supports complex and messy workloads\n",
    "* **Scales up**: Runs resiliently on clusters with 100s of nodes\n",
    "* **Scales down**: Trivial to set up and run on a laptop in a single process\n",
    "* **Responsive**: Designed with interactive computing in mind it provides rapid feedback and diagnostics to aid humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dask Computational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parallel programming with task scheduling\n",
    "* Familiar abstractions for executing tasks in parallel on data that doesn't fit into memory\n",
    "  * Arrays, DataFrames\n",
    "* Task graphs\n",
    "  * Representation of a parallel computation\n",
    "* Scheduling\n",
    "  * Executes task graphs in parallel on a single machine using threads or processes\n",
    "  * Preliminary support for parallel execution using `dask.distributed`\n",
    "    * Workflows for the distributed scheduler would be quite different than those presented below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "* If you don't have a big data problem, don't use a big data tool\n",
    "* Many of the below examples could easily be handled in-memory with some better choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"http://dask.pydata.org/en/latest/_images/collections-schedulers.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Subset of ndarray interface using blocked algorithms\n",
    "* Dask array complements large on-disk array stores like HDF5, NetCDF, and BColz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(\"http://dask.pydata.org/en/latest/_images/dask-array-black-text.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Arithmetic and scalar mathematics, `+, *, exp, log, ...`\n",
    "* Reductions along axes, `sum(), mean(), std(), sum(axis=0), ...`\n",
    "* Tensor contractions / dot products / matrix multiply, `tensordot`\n",
    "* Axis reordering / transpose, `transpose`\n",
    "* Slicing, `x[:100, 500:100:-2]`\n",
    "* Fancy indexing along single axes with lists or numpy arrays, `x[:, [10, 1, 5]]`\n",
    "* The array protocol `__array__`\n",
    "* Some linear algebra `svd, qr, solve, solve_triangular, lstsq`\n",
    "\n",
    "[Full API Documentation](http://dask.pydata.org/en/latest/array-api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The idea of the `chunk` is important and has performance implications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = da.arange(25, chunks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask operates on a delayed computation model\n",
    "* It builds up an expression of the computation in chunks\n",
    "* Creates a **Task Graph** that you can explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.dask.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can execute the graph by using **`compute`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As an example of the `__array__` protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can control the scheduler backend that is used by `compute`\n",
    "* These choices can be important in a few situations\n",
    "  * Debugging\n",
    "  * Fast tasks\n",
    "  * Cross-task communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dask.get` is an alias for the synchronous backend. Useful for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronous Queue Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute(get=dask.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threaded Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `dask.threaded.get` is the default\n",
    "* Uses a thread pool backend\n",
    "* A thread is the smallest unit of work that an OS can schedule\n",
    "  * Threads are \"lightweight\"\n",
    "* They execute within the same process and thus shares the same memory and file resources ([everything is a file](https://en.wikipedia.org/wiki/Everything_is_a_file) in unix)\n",
    "* Limitations\n",
    "  * Limited by the Global Interpreter Lock (GIL)\n",
    "    * A GIL means that only one thread can execute at the same time\n",
    "  * Pure python functions likely won't show a speed-up (with a few exceptions)\n",
    "    * C code can release the GIL\n",
    "    * I/O tasks are not blocked by the GIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default, dask will use as many threads as there are logical processors on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backend that uses multiprocessing\n",
    "* Uses a process pool backend\n",
    "  * On unix-like system this is a system call to `fork`\n",
    "  * Calling `fork` creates a new child process which is a *copy*(-on-write) of the parent process\n",
    "  * Owns its own resources. This is \"heavy\"\n",
    "* Limitations\n",
    "  * Relies on serializing objects for the workers (slow and error prone)\n",
    "  * Workers must communicate through parent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.multiprocessing\n",
    "y.compute(get=dask.multiprocessing.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributed Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is part of the `dask.distributed` library\n",
    "* Distributes work over the network across machines using web sockets and an asychronous web framework for Python (tornado)\n",
    "  * Some recent additions make this work for, e.g., distributed DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocked Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask works on arrays by executing blocked algorithms on chunks of data\n",
    "* For example, consider taking the mean of a billion numbers. We might instead break up the array into 1,000 chunks, each of size 1,000,000, take the sum of each chunk, and then take the sum of the intermediate sums and divide this by the total number of observations.\n",
    "* the result (one sum on one billion numbers) is performed by many smaller results (one thousand sums on one million numbers each, followed by another sum of a thousand numbers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "f = h5py.File(os.path.join('..', 'data', 'random.hdf5'))\n",
    "dset = f['/x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If were to implement this ourselves it might look like this\n",
    "1. Computing the sum of each 1,000,000 sized chunk of the array\n",
    "2. Computing the sum of the 1,000 intermediate sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = []\n",
    "for i in range(0, 1000000000, 1000000):\n",
    "    chunk = dset[i: i + 1000000]\n",
    "    sums.append(chunk.sum())\n",
    "\n",
    "total = np.sum(sums)\n",
    "print(total / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask does this for you and uses the backend scheduler to do so in parallel\n",
    "* Create a dask array from an array-like structure (any object that implements numpy-like slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = da.from_array(dset, chunks=(1000000, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* x looks and behaves much like a numpy array\n",
    "  * Arithmetic, slicing, reductions\n",
    "* Use tab-completion to look at the methods of `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `dask.array.random.normal` to create a 20,000 x 20,000 array $X ~ \\sim N(10, .1)$ with `chunks` set to `(1000, 1000)`\n",
    "\n",
    "Take the mean of every 100 elements along axis 0.\n",
    "\n",
    "*Hint*: Recall you can slice with the following syntax [start:end:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/dask_array.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs. NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your performance may vary. If you attempt the NumPy version then please ensure that you have more than 4GB of main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x.mean(axis=0)[::100] \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster and needs only MB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "y = x.mean(axis=0)[::100] \n",
    "y.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dask implements a few linear algebra functions that are paraellizable\n",
    "* `da.linalg.qr`\n",
    "* `da.linalg.cholesky`\n",
    "* `da.linalg.svd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parallel lists for semi-structured data\n",
    "  * Nested, variable length, heterogenously typed, etc.\n",
    "  * E.g., JSON blobs or text data\n",
    "* Anything that can be represented as a large collection of generic Python objects \n",
    "* Mainly for cleaning and processing\n",
    "  * I.e., usually the first step in a workflow \n",
    "* Bag implements a number of useful methods for operation on sequences like `map`, `filter`, `fold`, `frequencies` and `groupby`\n",
    "* Streaming computation on top of generators\n",
    "* Bags use the multiprocessing backend by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the accounts data we created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag = db.read_text(os.path.join('..', 'data', 'accounts.*.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using map to process the lines in the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "js = bag.map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = js.pluck('name').frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `filter` and `take` all of the transactions for the first five users named \"Alice\"\n",
    "* Define a function `count_transactions` that takes a dictionary from `accounts` and returns a dictionary that holds the `name` and a key `count` that is the number of transactions for that user id.\n",
    "* Use `filter` to get the accounts where the user is named Alice and `map` the function you just created to get the number of transactions for each user named Alice. `pluck` the count and display the first 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/bag_alice.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy / FoldBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Groupby collects items in your collection so that all items with the same value under some function are collected together into a key-value pair.\n",
    "* This requires a full on-disk shuffle and is *very* inefficient\n",
    "* You almost never want to do this in a real workflow if you can avoid it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence(['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank'])\n",
    "b.groupby(len).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Group by evens and odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence(list(range(10)))\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by eevens and odds and take the largest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.groupby(lambda x: x % 2).map(lambda k, v: (k, max(v))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FoldBby, while harder to grok, is much more efficient\n",
    "* This does a streaming combined groupby and reduction\n",
    "* Familiar to Spark users as the `combineByKey` method on `RDD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using foldby you provide\n",
    "\n",
    "1. A key function on which to group elements\n",
    "2. A binary operator such as you would pass to reduce that you use to perform reduction per each group\n",
    "3. A combine binary operator that can combine the results of two reduce calls on different parts of your dataset.\n",
    "\n",
    "Your reduction must be associative. It will happen in parallel in each of the partitions of your dataset. Then all of these intermediate results will be combined by the combine binary operator.\n",
    "\n",
    "This is just what we saw in `sum` above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `functools.reduce` works like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "values = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func(acc, y):\n",
    "    print(acc)\n",
    "    print(y)\n",
    "    print()\n",
    "    return acc + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functools.reduce(func, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.foldby(lambda x: x % 2, binop=max, combine=max).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the accounts data above, find the number of people with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = js.foldby(key='name',\n",
    "                   binop=lambda total, x: total + 1,\n",
    "                   initial=0,\n",
    "                   combine=lambda a, b: a + b,            \n",
    "                   combine_initial=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    result = counts.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Compute the total amounts for each name\n",
    "* First, create a function that computes the total for each user id\n",
    "* Change the above example to accumulate the total amount instead of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/bag_foldby.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* subset of the pandas API\n",
    "* Good for analyzing heterogenously typed tabular data arranged along an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\", width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trivially parallelizable operations (fast)**:\n",
    "\n",
    "  * Elementwise operations: `df.x + df.y, df * df`\n",
    "  * Row-wise selections: `df[df.x > 0]`\n",
    "  * Loc: `df.loc[4.0:10.5]`\n",
    "  * Common aggregations: `df.x.max(), df.max()`\n",
    "  * Is in: `df[df.x.isin([1, 2, 3])]`\n",
    "  * Datetime/string accessors: `df.timestamp.month`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleverly parallelizable operations (fast)**:\n",
    "\n",
    "  * groupby-aggregate (with common aggregations): `df.groupby(df.x).y.max(), df.groupby('x').max()`\n",
    "  * value_counts: `df.x.value_counts()`\n",
    "  * Drop duplicates: `df.x.drop_duplicates()`\n",
    "  * Join on index: `dd.merge(df1, df2, left_index=True, right_index=True)`\n",
    "  * Join with Pandas DataFrames: `dd.merge(df1, df2, on='id')`\n",
    "  * Elementwise operations with different partitions / divisions: `df1.x + df2.y`\n",
    "  * Datetime resampling: `df.resample(...)`\n",
    "  * Rolling averages: `df.rolling(...)`\n",
    "  * Pearson Correlations: `df[['col1', 'col2']].corr()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operations requiring a shuffle (slow-ish, unless on index)**\n",
    "\n",
    "  * Set index: `df.set_index(df.x)`\n",
    "  * groupby-apply (with anything): `df.groupby(df.x).apply(myfunc)`\n",
    "  * Join not on the index: `dd.merge(df1, df2, on='name')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Full DataFrame API](http://dask.pydata.org/en/latest/dataframe-api.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"../data/NationalFoodSurvey/NFS*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `DataFrame.head` is one operation that is not lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By default the data is partitioned by the file\n",
    "* In our case, this is good. The files have a natural partition\n",
    "* When this is not the case, you must do a disk-based shuffle which is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.known_divisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to set the partition explicitly to `styr` to make some operations more performant\n",
    "* Partitions are denoted by the left-side of the bins for the partitions.\n",
    "* The final value is assumed to be the inclusive right-side for the last bin.\n",
    "\n",
    "\n",
    "So\n",
    "\n",
    "```\n",
    "[1974, 1975, 1976]\n",
    "```\n",
    "\n",
    "Would be 2 partitions. The first contains 1974. The second contains 1975 and 1976. To get three partitions, one for the final observation, duplicate it.\n",
    "\n",
    "```\n",
    "[1974, 1975, 1976, 1976]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = list(range(1974, 2001)) + [2000]\n",
    "\n",
    "df = df.set_partition('styr', divisions=partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.known_divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.divisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Nothing yet is loaded in to memory\n",
    "* Meta-information from pandas is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In addition to the (supported) pandas DataFrame API, dask provides a few more convenient methods\n",
    "    * `DataFrame.categorize`\n",
    "    * `DataFrame.map_partions`\n",
    "    * `DataFrame.get_division`\n",
    "    * `DataFrame.repartition`\n",
    "    * `DataFrame.set_partition`\n",
    "    * `DataFrame.to_{bag|castra}`\n",
    "    * `DataFrame.visualize`\n",
    "    \n",
    "* A few methods have a slightly different API\n",
    "\n",
    "    * `DataFrame.apply`\n",
    "    * `GroupBy.apply`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2000 = df.get_division(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What food group was consumed the most times in 2000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2000.set_index('minfd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE: We could speed up subsequent operations by setting partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp = df2000.groupby('minfd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = grp.apply(len, columns='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There isn't (yet) support for idxmin/idxmax.\n",
    "* Turn it into a Series first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minfd = size.compute().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the pre-processed mapping across food grouping variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_mapping = pd.read_csv(\"../data/NationalFoodSurvey/food_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas provides the efficient `isin` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_mapping.ix[food_mapping.minfd.isin([minfd])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* What was the most consumed food group in 1974?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/nfs_most_purchased.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map_partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Map partitions does what you might expect\n",
    "* Maps a function across partitions\n",
    "* Let's calculate the most frequently purchase food group for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent_food(partition):\n",
    "    # partition is a pandas.DataFrame\n",
    "    grpr = partition.groupby('minfd')\n",
    "    size = grpr.size()\n",
    "    minfd = size.idxmax()\n",
    "    idx = food_mapping.minfd.isin([minfd])\n",
    "    description = food_mapping.ix[idx].minfddesc.iloc[0]\n",
    "    year = int(partition.styr.iloc[0])\n",
    "    return year, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnfd_year = df.map_partitions(most_frequent_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnfd_year.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zip(mnfd_year.compute(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Within each year, group by household `minfd` and calculate daily per capita consumption of each food group. Hint, you want to use `map_partitions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Solution here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load solutions/average_consumption.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside on Storage Formats: Thinking about Computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* csv is a terrible format (performance-wise)\n",
    "* compressed csv is not much better\n",
    "* memory-bound workloads\n",
    "  * [Why Modern CPUs are Starving and What Can Be Done about It](http://www.blosc.org/docs/StarvingCPUs-CISE-2010.pdf)\n",
    "  * [Latency numbers every programmer should know](http://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html)\n",
    "* trend towards columnar-compressed storage formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blosc\n",
    "\n",
    "* Meta-compression format for (binary) data\n",
    "* Cache-aware\n",
    "* Can be faster query data on disk with blosc (bcolz) than pandas in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image('images/bcolz_bench.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Examples and Tutorials](http://dask.pydata.org/en/latest/examples-tutorials.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
